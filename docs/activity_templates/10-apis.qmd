---
title: "Data acquisition: APIs"
sidebar: false
---

```{r message=FALSE}
library(tidyverse)
```

# Learning goals

After this lesson, you should be able to:

- Explain what an API is
- Set up an API key for a public API
- Develop comfort in using a wrapper package or URL-method of calling a web API
- Recognize the structure in a URL for a web API and adjust for your purposes
- Explore and subset complex nested lists

<br>

You can download a template Quarto file to start from [here](activity_templates/10-apis.qmd). Save this template within the following directory structure:

- `your_course_folder`
    - `apis`
        - `code`
            - `10-apis.qmd`


<br><br><br>


# APIs

In this lesson you'll learn how to collect data from websites such as The New York Times, Zillow, and Google. While these sites are primarily known for the information they provide to humans browsing the web, they (along with most large websites) also provide information to computer programs.

**API** stands for **Application Programming Interface**, and this term describes a general class of tool that allows computers, rather than humans, interact with an organization's data.

- Humans use browsers such as Firefox or Chrome to navigate the web. Behind the scenes, our browsers communicate with web servers using a technology called [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) or Hypertext Transfer Protocol to get information that is formatted into the display of a web page.
- Programming languages such as R can also use HTTP to communicate with web servers. We'll see next time how we can use R to "scrape" data from almost any static web page. However, it's easiest to interact with websites that are specifically designed to communicate with programs. These [Web APIs](https://en.wikipedia.org/wiki/Web_API), or Web Application Programming Interfaces, focus on transmitting raw data, rather than images, colors, or other appearance-related information that humans interact with when viewing a web page.

A large variety of web APIs provide data accessible to programs written in R (and almost any other programming language!). Almost all reasonably large commercial websites offer APIs. Todd Motto has compiled an excellent list of [Public Web APIs](https://github.com/toddmotto/public-apis) on GitHub. Browse the list to see what kind of information is available.


## Wrapper packages

Extra resources:

1. [NY Times API](https://developer.nytimes.com/?mcubz=3)
2. [NY Times Blog post announcing the API](https://open.blogs.nytimes.com/2009/02/04/announcing-the-article-search-api/?mcubz=3&_r=0)
3. [Working with the NY Times API in `R`](https://www.storybench.org/working-with-the-new-york-times-api-in-r/)
4. [nytimes pacakge for accessing the NY Times' APIs from `R`](https://github.com/mkearney/nytimes)
5. [Video showing how to use the NY Times API](https://www.youtube.com/watch?v=3at3YTAFbxs)
6. [rOpenSci](https://ropensci.org/packages/) has a good collection of wrapper packages

In R, it is easiest to use Web APIs through a **wrapper package**, an R package written specifically for a particular Web API. The R development community has already contributed wrapper packages for most large Web APIs. To find a wrapper package, search the web for "R Package" and the name of the website. For example, a search for "R Reddit Package" returns [RedditExtractor](https://cran.r-project.org/web/packages/RedditExtractoR/index.html) and a search for "R Weather.com Package" surfaces [weatherData](https://ram-n.github.io/weatherData/).

This activity will build on the [New York Times Web API](https://developer.nytimes.com/), which provides access to news articles, movie reviews, book reviews, and many other data. Our activity will specifically focus on the [Article Search API](https://developer.nytimes.com/docs/articlesearch-product/1/overview), which finds information about news articles that contain a particular word or phrase. 

We will use the [nytimes](https://github.com/mkearney/nytimes) wrapper package that provides functions for some (but not all) of the NYTimes APIs. You can install the package by running the following in the Console:

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("mkearney/nytimes")
```

Next, take a look at the Article Search API example on the [package website](https://github.com/mkearney/nytimes) to get a sense of the syntax.

**Exercise:** What do you think the `nyt_search()` function below does? How does it communicate with the NY Times? Where is the data about articles stored?

```{r eval=FALSE}
res <- nyt_search(q = "gamergate", n = 20, end_date = "20150101")
```

To get started with the NY Times API, you must [register and get an **authentication key**](https://developer.nytimes.com/accounts/create). Signup only takes a few seconds, and it lets the New York Times make sure nobody abuses their API for commercial purposes. It also **rate limits** their API and ensures programs don't make too many requests per day. For the NY Times API, this limit is 1000 calls per day. Be aware that most APIs do have rate limits --- especially for their free tiers.

Once you have signed up, verified your email, log back in to https://developer.nytimes.com. Under your email address, click on Apps and Create a new App (call it First API) and enable Article Search API, then press Save. This creates an **authentication key**, which is a 32 digit string with numbers and the letters a-e. 

Store this in a variable as follows (this is just an example ID, not an actual one):

```{r}
# Change value to your personal API key
times_key <- "c935b213b2dc1218050eec976283dbbd"
```

Now, let's use the key to issue our first API call. We'll adapt the code we see in the vignette to do what we need.

```{r}
library(nytimes)

# Tell nytimes what our API key is
Sys.setenv(NYTIMES_KEY = times_key)

# Issue our first API call
res <- nyt_search(q = "gamergate", n = 20, end_date = "20150101")

# Convert response object to data frame
res <- as.data.frame(res)
```

Something magical just happened. Your computer sent a message to the New York Times and asked for information about 20 articles about [Gamergate](https://en.wikipedia.org/wiki/Gamergate_controversy) starting at January 1, 2015 and going backwards in time. Thousands of public Web APIs allow your computer to tap into almost any piece of public digital information on the web. 

Let's take a peek at the structure of the results:

```{r }
colnames(res)
head(res)
```


## Accessing web APIs directly

Wrapper packages such as `nytimes` provide a convenient way to interact with Web APIs. However, many Web APIs have incomplete wrapper packages, or no wrapper package at all. Fortunately, most Web APIs share a common structure that `R` can access relatively easily. There are two parts to each Web API:

- The **request**: this amounts to calling a function that gets sent to a web server
    - In our `nyt_search(q = "gamergate", n = 20, end_date = "20150101")` example, the `q`, `n`, and `end_date` are arguments to an article search function.
- The **response**: the web server computes the result to the function call and returns the response
    - The web server uses runs a search with the `q`, `n`, and `end_date` arguments to get the search results.

As mentioned earlier, a Web API call differs from a regular function call in that the request is sent over the Internet to a web server, which performs the computation and calculates the return result, which is sent back over the Internet to the original computer.

### Web API requests

For a deeper dive, consult the following readings:

1. [Understanding URLs](https://www.tutorialspoint.com/html/understanding_url_tutorial.htm)
2. [urltools Vignette](https://cran.r-project.org/web/packages/urltools/vignettes/urltools.html)

The request for a Web API call is usually encoded through the [URL](https://www.tutorialspoint.com/html/understanding_url_tutorial.htm) (short for uniform resource locator), the web address associated with the API's web server. Let's look at the URL associated with the first `nytimes` `nyt_search` example we did. Open the following URL in your browser (you should replace `MY_KEY` with the API key you were given earlier).

    http://api.nytimes.com/svc/search/v2/articlesearch.json?q=gamergate&api-key=MY_KEY
    
The text you see in the browser is the response data. We'll talk more about that in a bit. Right now, let's focus on the structure of the URL. You can see that it has a few parts:

- `http://` --- The **scheme**, which tells your browser or program how to communicate with the web server. This will typically be either `http:` or `https:`.
- `api.nytimes.com` --- The **hostname**, which is a name that identifies the web server that will process the request.
- `/svc/search/v2/articlesearch.json` --- The **path**, which tells the web server what function you would like to call.
- `?q=gamergate&api-key=MY_KEY` --- The **query parameters**, which provide the parameters for the function you would like to call. Note that the query can be thought of as a table, where each row has a key and a value (known as a key-value pair). In this case, the first row has key `q` and value `gamergate` and the second row has value `MY_KEY`. The query parameters are preceded by a `?`. Rows in the key-value table are separated by `&`, and individual key-value pairs are separated by an `=`.

key      value
----     ------
q        gamergate
api-key  MY_KEY

Typically, each of these URL components will be specified in the API documentation. Sometimes, the scheme, hostname, and path (`http://api.nytimes.com/svc/search/v2/articlesearch.json`) will be referred to as the **[endpoint](https://en.wikipedia.org/wiki/Web_API#Endpoints)** for the API call.

We will use the `urltools` package to build up a full URL from its parts. Start by creating a string with the endpoint. Then add the parameters one by one using `param_set` and `url_encode`:

```{r}
library(urltools)

url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("gamergate"))
url <- param_set(url, "api-key", url_encode(times_key))
url
```

Copy and paste the resulting URL into your browser to see what the NY Times response looks like!

You may be wondering why we need to use `param_set()` and `url_encode()` instead of writing the full URL by hand. The following exercise will illustrate why we need to be careful.

**Pair programming exercise:** Work through the two exercises below in pairs (or triples as needed). Whoever has visited more countries in their lifetime will be driver first.

**Exercise:** Write a function that generalizes our URL construction steps above so that the user can input any search query (`q`).

- Use your function to create a URL that finds articles related to `Ferris Bueller's Day Off` (note the apostrophe). What is interesting about how the title appears in the URL?
- Repeat for the query `Penn & Teller` (make sure you use the punctuation mark `&`). What do you notice?

Take a look at the Wikipedia page describing [percent encoding](https://en.wikipedia.org/wiki/Percent-encoding). Explain how the process works in your own words.

**Exercise:** Write out the pseudocode for a function that takes a data frame of arbitrarily many key-value pairs and constructs the URL. Then write the function itself. Example data frame of key-value pairs:

```{r}
key_val_pairs <- tibble(
    key = c("q", "api-key", "begin_date", "end_date"),
    value = c("economy", "API_KEY", "20230101", "20231001")
)
key_val_pairs
```




### Web API responses

For a deeper dive, consult the following readings:

1. [A Non-Programmer's Introduction to JSON](https://blog.scottlowe.org/2013/11/08/a-non-programmers-introduction-to-json/)
2. [Getting Started With JSON and jsonlite](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html)
3. [Fetching JSON data from REST APIs](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-apis.html)

Let's discuss the structure of the web **response**, the return value of the Web API function. Web APIs generate string responses. If you visited the earlier New York Times API link in your browser, you would be shown the string response from the New York Times web server:

```
{"status":"OK","copyright":"Copyright (c) 2023 The New York Times Company. All Rights Reserved.","response":{"docs":[{"abstract":"Who would have guessed that magic’s most recognizable buddy pair would produce the classiest reality show on television?","web_url":"https://www.nytimes.com/2019/11/26/magazine/letter-of-recommendation-penn-teller-fool-us.html","snippet":"Who would have guessed that magic’s most recognizable buddy pair would produce the classiest reality show on television?","lead_paragraph":"“Penn & Teller: Fool Us” is a reality-TV competition shown on the CW, which is a broadcast network, which is something like a streaming service that’s always on. The show was recently renewed for its seventh season. The only other person I know who watches it is a skilled amateur magician and general magic geek who lives in Chicago. For him, the show is a chance to be exposed to some of the world’s greatest magicians and get an insight into their arcane techniques. For me, who doesn’t particularly like magic and has no intention of trying to do it, the show has a different appeal: It makes me a better person.","print_section":"MM","print_page":"24","source":"The New York Times","multimedia":
```

If you stared very hard at the above response, you may be able to interpret it. However, it would be much easier to interact with the response in some more structured, programmatic way. The vast majority of Web APIs, including the New York Times, use a standard called JSON (Javascript Object Notation) to take data and encode it as a string.

To understand the structure of JSON, take the NY Times web response in your browser, and copy and paste it into this online [JSON formatter](https://jsonformatter.curiousconcept.com/). The formatter will add newlines and tabs to make the data more human-readable. You'll see the following:

```
{
   "status":"OK",
   "copyright":"Copyright (c) 2023 The New York Times Company. All Rights Reserved.",
   "response":{  
      "docs":[  
      
        # A HUGE piece of data, with one object for each of the result articles
        
      ],
      "meta":{
         "hits":1755,
         "offset":0,
         "time":51
      }
   }
}     
```

You'll notice a few things in the JSON above:

- Strings are enclosed in double quotes, for example `"status"` and `"OK"`.
- Numbers are written plainly, like `2350` or `72`.
- Some data is enclosed in square brackets `[` and `]`. These data containers can be thought of as R lists.
- Some data is enclosed in curly braces `{` and `}`. These data containers are called **Objects**. An object can be thought of as a single case or observation in a table.
    - The columns or variables for the observation appear as **keys** on the left (`hits`, `offset`, etc.).
    - The **values** appear after the specific key separated by a colon (`2350`, and `0`, respectively).

Thus, we can think of the `meta` object above as:

hits    offset    time
------  --------  ------
1755    0         51

Let's repeat the NY Times search for "gamergate", but this time we will perform the Web API call by hand instead of using the `nytimes` wrapper package. We will use the `jsonlite` package to retrieve the response from the web server and turn the string response into an `R` object. The `fromJson` function sends our request out over and across the web to the NY Times web server, retrieves it, and turns it from a JSON-formatted string into R data.

```{r}
library(jsonlite)

# Rebuild the URL
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("gamergate"))
url <- param_set(url, "api-key", url_encode(times_key))

# Send the request to the webserver over the Internet and
# retrieve the JSON response. Turn the JSON response into an
# R Object.
gamergate_json <- fromJSON(url)
```

`gamergate_json` is a **list**. A list is a useful structure for storing elements of different types. Data frames are special cases of lists where each list element has the same length (but where the list elements have different classes).

Lists are a very flexible data structure but can be very confusing because list elements can be lists themselves!

We can explore the structure of a list in two ways:

- Entering `View(list_object)` in the Console. The triangle buttons on the left allow you to toggle dropdowns to explore list elements.
- Using the `str()` (structure) function.

**Exercise:** Explore the information in the `gamergate_json` using both `View()` and `str()`. When using `str()`, look up the documentation and experiment with the `max.level` and `vec.len` arguments to control how the output is displayed. Look back and forth between the `View()` and `str()` output to find correspondences in how object structure is displayed.

--------------------------------------------------

We can access elements of a list in three ways:

- By *position* with double square brackets `[[`:

```{r}
# This gets the first element of the list
gamergate_json[[1]]
```


- By *name* with double square brackets `[[`: (note that list elements are not always named, so this won't always be possible)

```{r}
# Accessing by name directly
gamergate_json[["status"]]

# Accessing via a variable
which_element <- "status"
gamergate_json[[which_element]]
```

- By *name* with a dollar sign `$`: (Helpful tip: For this mode of access, RStudio allows tab completion to fill in the full name)

```{r}
gamergate_json$status
```

We can retrieve these *nested* attributes by sequentially accessing the object keys from the outside in. For example, the `meta` element would be accessed as follows:

```{r}
gamergate_json$response$meta
```


**Exercise:** In the `gamergate_json` object, retrieve the data associated with:

- the `copyright` key
- the number of `hits` (number of search results) within the `meta` object
- the abstracts and leading paragraphs of the articles found in the search


**Exercise: Your own article search** 

a. Select your own article search query (any topic of interest to you). You may want to play with NY Times online search or the [API web search console](https://developer.nytimes.com/docs/articlesearch-product/1/routes/articlesearch.json/get) to find a query that is interesting, but not overly popular. You can change any part of the query you would like. Your query should have at least 30 matches.

b. Retrieve data for the first three pages of search results from the article search API, and create a data frame that joins together the `docs` data frames for the three pages of results. (Read the "Multiple pages of search results" section below to see how to combine multiple pages of results with `bind_rows()`.)
 
c. Make a plot of the number of search results over time in your result set (likely by day or month). This will involve some data wrangling. It will be helpful to have the `lubridate` [reference page](https://lubridate.tidyverse.org/reference/index.html) open.


### Multiple pages of search results

Here is some code to generate queries on NY Times articles about the Red Sox. It fetches the first thirty entries in batches of 10.

```{r}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Red Sox"))
url <- param_set(url, "api-key", url_encode(times_key))
url <- param_set(url, "page", 0)
Sys.sleep(1)
res1 <- fromJSON(url)

# This pauses for 1 second.
# It is required when knitting to prevent R from issuing too many requests to
# The NYT API at a time. If you don't have it you will get an error that
# says "Too Many Requests (429)"
Sys.sleep(1)
url <- param_set(url, "page", 1)
res2 <- fromJSON(url)

Sys.sleep(1)
url <- param_set(url, "page", 2)
res3 <- fromJSON(url)

docs1 <- res1$response$docs
docs2 <- res2$response$docs
docs3 <- res3$response$docs
```

Each of these docs variables is a table with ten entries (articles) and the same 18 variables:

```{r}
colnames(docs1)
colnames(docs2)
colnames(docs3)
```

Now we want to stack the tables on top of each other to get a single table with 30 rows and 18 variables. We can use:

```{r}
bind_rows(docs1,docs2,docs3)
```


<br><br><br>


# Extra practice: Create your own public API visualization

Browse [toddomotos' list of Public APIs](https://github.com/toddmotto/public-apis) and [abhishekbanthia's list of Public APIs](https://github.com/abhishekbanthia/Public-APIs). Select one of the APIs from the list. Here are a few criteria you should consider:

- Use the JSON approach we illustrated above; not all APIs support JSON. (If you want to use an API that does not support JSON, you can check if there is an `R` wrapper package.)
- Stay away from APIs that require OAuth for Authorization unless you are prepared for extra work before you get data! Most of the large social APIs (Facebook, LinkedIn, Twitter, etc.) require OAuth. toddomoto's page lists this explicitly, but you'll need to dig a bit if the API is only on abhishekbanthia's list.
- You will probably need to explore several different APIs before you find one that works well for your interests and this exercise.
- Beware of the `rate limits` associated with the API you choose. These determine the maximum number of API calls you can make per second, hour or day. Though these are not always officially published, you can find them by Google (for example) `GitHub API rate limit`. If you need to slow your program down to meet the API insert calls to `Sys.sleep(1)` as is done in the example below.
- Sketch out one interesting visualization that relies on the public API you selected earlier. Make sure the exact data you need is available. If it's not, try a new visualization or API.
- If a wrapper package is available, you may use it, but you should also try to create the request URL and retrieve the JSON data using the techniques we showed earlier, without the wrapper package.
- Visualize the data you collected and describe the results.

